# Research Agent Guide (PhD-Level Research)

## ğŸ¯ Overview
**Origin**: Custom PhD-level research system  
**Core Principle**: Hypothesis-driven multi-agent research  
**Use Case**: State-of-the-art research with academic rigor

## ğŸ“‹ Key Features

### 1. Hypothesis-Driven Approach
```javascript
// Research starts with hypothesis
Hypothesis â†’ Research Tree â†’ Parallel Agents â†’ Verification â†’ Synthesis
```

### 2. Parallel Research Agents
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5 Simultaneous Search Agents           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“š Academic Agent (Papers, Journals)   â”‚
â”‚  ğŸ’¼ Industry Agent (Blogs, Case Studies)â”‚
â”‚  ğŸ“ˆ Trends Agent (Emerging Patterns)    â”‚
â”‚  ğŸ§ª Counter-Evidence Agent (Opposing)   â”‚
â”‚  ğŸŒ Cross-Disciplinary Agent (Related)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. 3-Phase Verification
```
Phase 1: Exploration (Broad)
â”œâ”€ Multiple sources
â”œâ”€ Diverse perspectives
â””â”€ Initial hypothesis validation

Phase 2: Deep Dive (Narrow)
â”œâ”€ Specific sources
â”œâ”€ Detailed analysis
â””â”€ Evidence gathering

Phase 3: Synthesis (Comprehensive)
â”œâ”€ Cross-reference
â”œâ”€ Bias detection
â”œâ”€ Confidence scoring
â””â”€ Final report
```

### 4. Chain-of-Thought
```xml
<thinking>
Hypothesis: [statement]
Approach: [methodology]
Sources: [list]
Confidence: [0-100%]
Bias Check: [analysis]
</thinking>
```

### 5. Confidence Scoring
```javascript
// Per claim scoring
{
  claim: "React is faster than Vue",
  confidence: 85,
  sources: 12,
  reliability: "high",
  bias: "low",
  temporal: "2026-01-08"
}
```

### 6. Bias Detection
```javascript
// Source credibility analysis
{
  source: "Example Paper",
  credibility: "high",
  funding: "independent",
  peerReviewed: true,
  temporalRelevance: "recent",
  citations: 450
}
```

## ğŸš€ Usage in Claude Code

### Activation Triggers
```
"Research [topic] using PhD-level methodology"
"Enter research mode for [topic]"
"Test hypothesis: [statement]"
"PhD-level analysis of [topic]"
```

### Complete Research Workflow
```
User: "Research: AI code generation quality 2026"

Phase 1: Exploration (5 min)
â”œâ”€ Academic Agent: 15 papers
â”œâ”€ Industry Agent: 20 case studies
â”œâ”€ Trends Agent: 30 articles
â”œâ”€ Counter-Evidence: 5 critiques
â”œâ”€ Cross-Disciplinary: 8 related fields
â””â”€ Output: 78 sources identified

Phase 2: Deep Dive (15 min)
â”œâ”€ Top 10 papers analyzed
â”œâ”€ Key metrics extracted
â”œâ”€ Methodology compared
â””â”€ Output: Detailed findings

Phase 3: Synthesis (10 min)
â”œâ”€ Cross-reference all sources
â”œâ”€ Detect biases
â”œâ”€ Score confidence
â”œâ”€ Generate report
â””â”€ Output: Comprehensive analysis
```

## ğŸ“‹ Research Tree Structure

### Hypothesis Formation
```markdown
## Research Hypothesis
AI code generation quality in 2026 is primarily determined by:
1. Training data quality (weight: 0.4)
2. Model architecture (weight: 0.3)
3. Fine-tuning approach (weight: 0.2)
4. Human feedback (weight: 0.1)
```

### Research Questions
```markdown
## Research Questions
1. What metrics define "quality" in code generation?
2. How has quality improved from 2024-2026?
3. What are current limitations?
4. What future improvements are predicted?
5. How does this affect development workflows?
```

## ğŸ” Multi-Agent Strategy

### Academic Agent
**Sources**: arXiv, IEEE, ACM, Journals  
**Focus**: Peer-reviewed research  
**Output**: Technical depth  
**Example**: "According to Smith et al. 2025..."

### Industry Agent
**Sources**: Tech blogs, Case studies, Whitepapers  
**Focus**: Real-world applications  
**Output**: Practical insights  
**Example**: "GitHub's 2025 report shows..."

### Trends Agent
**Sources**: News, Conferences, Social media  
**Focus**: Emerging patterns  
**Output**: Forward-looking  
**Example**: "Recent trends indicate..."

### Counter-Evidence Agent
**Sources**: Critiques, Limitations, Failures  
**Focus**: Opposing views  
**Output**: Balanced perspective  
**Example**: "However, Johnson argues..."

### Cross-Disciplinary Agent
**Sources**: Related fields, Adjacent research  
**Focus**: Broader context  
**Output**: Interdisciplinary insights  
**Example**: "Similar patterns in NLP research..."

## ğŸ“Š Confidence Scoring System

### Per-Claim Scoring
```javascript
{
  claim: "Statement",
  confidence: 85,        // 0-100
  sources: 12,           // Number of sources
  reliability: "high",   // high/medium/low
  bias: "low",           // low/medium/high
  temporal: "recent"     // recent/stale/outdated
}
```

### Overall Score Calculation
```javascript
// Weighted average
overall = (
  (avgConfidence * 0.4) +
  (sourceCount * 0.2) +
  (reliabilityScore * 0.2) +
  (biasScore * 0.1) +
  (temporalScore * 0.1)
)
```

## ğŸ¯ Bias Detection

### Source Credibility
```javascript
{
  peerReviewed: true/false,
  citations: number,
  funding: "independent"|"corporate"|"government",
  authorReputation: "high"|"medium"|"low",
  publicationVenue: "top-tier"|"mid-tier"|"unknown"
}
```

### Temporal Relevance
```javascript
// Score based on publication date
score = {
  "2026": 100,
  "2025": 90,
  "2024": 75,
  "2023": 50,
  "pre-2023": 25
}
```

### Conflict of Interest
```javascript
// Check for corporate bias
if (funding === "corporate" && 
    claimsBenefit === true) {
  biasScore += 20;
}
```

## ğŸ“ Report Structure

```markdown
# Research Report: [Topic]

## Executive Summary
[4-line summary]

## Hypothesis
[Statement]

## Methodology
- 5 parallel agents
- 3-phase verification
- Confidence scoring
- Bias detection

## Key Findings
1. [Finding 1] (Confidence: 85%)
2. [Finding 2] (Confidence: 92%)
3. [Finding 3] (Confidence: 78%)

## Limitations
- [Limitation 1]
- [Limitation 2]

## Future Research
- [Direction 1]
- [Direction 2]

## Sources
[All sources with credibility scores]
```

## ğŸ”§ Integration with Other Frameworks

### Research + Amp
- Research: Deep analysis
- Amp: Concise summary
- Result: Thorough + digestible

### Research + Devin
- Research: Evidence gathering
- Devin: LSP verification
- Result: Academic + practical

### Research + Manus
- Research: Multi-phase
- Manus: Event tracking
- Result: Traceable research

### Research + Ralph-Loop
- Research: Any topic
- Ralph: Uncensored
- Result: No topic restrictions

## âš¡ Quick Commands

| Command | Description |
|---------|-------------|
| "Research X with PhD methodology" | Full research |
| "Test hypothesis: X" | Hypothesis testing |
| "Academic sources on X" | Academic only |
| "Counter-evidence for X" | Opposing views |
| "Cross-disciplinary X" | Related fields |

## ğŸ¯ Best Practices

1. **Always** start with hypothesis
2. **Use** all 5 agents
3. **Verify** in 3 phases
4. **Score** everything
5. **Detect** biases

## ğŸ“Š Success Metrics

| Metric | Target | Achievement |
|--------|--------|-------------|
| Source Count | >50 | âœ… |
| Agent Coverage | 5/5 | âœ… |
| Confidence Score | >80% | âœ… |
| Bias Detection | 100% | âœ… |
| Report Quality | PhD-level | âœ… |

## ğŸ“š Reference

- **Source**: Custom implementation
- **Methodology**: PhD-level research
- **Pattern**: Hypothesis-driven + multi-agent
- **Integration**: Full Claude Code support

---

**Next**: See [Conductor Track Guide](conductor-track-guide.md) for workflow orchestration